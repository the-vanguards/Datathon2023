

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
{
library(tidyverse)
library(reticulate) 
library(naniar)
compname <- Sys.info()
}

 
py_install("statsmodels")
# py_install("git+https://github.com/microsoft/dowhy.git")
py_install("dowhy", pip=T)
py_install("xgboost")
# py_install(c("xgboost","lightgbm","shap"))
py_install("lightgbm")
py_install("econml", pip = T)
py_install("numpy", pip = T)
py_install("scikit-learn", pip = T)
py_install("numba", pip = T)
py_install("causalml", pip = T)
py_install("IPython")
conda_remove("r-reticulate", packages = "lightgbm")
# py_install("pandas")
pd <- import("pandas")



```


# Arif's data
```{r}
# library(data.table)
df <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23/outputs/curr_final_v5.csv")


df <- read_csv("G:/My Drive/datathon23/outputs/curr_final_v3.csv") 
# df <- ("G:/My Drive/datathon23/outputs/curr_final_v3.csv")


# set.seed(1)
df1 <- df |>
  # slice_sample(prop = 0.5) |>
  mutate(mp = 0.098 * tidal/1000 * a2_resp_rate * (peep + insp_p),
         pfr = PaO2/FiO2*100) |>
  group_by(admissionid) |>
  mutate(across(.cols = a2_resp_rate:pfr, .fn = ~mean(.,na.rm = T))) |>
  left_join(dead, by = c("admissionid"))  |>
  # group_by(admissionid) |>
  mutate(total_vent = sum(is_iv, na.rm = T)/24,
         vent7 = 7 - total_vent,
         vent28 = 28 - total_vent,
         vfd7 = case_when(
           dead == 0 & vent7 > 0 ~ vent7,
           TRUE ~ 0
         ),
         vfd28 = case_when(
           dead == 0 & vent28 > 0 ~ vent28,
           TRUE ~ 0)) |> 
  drop_na() |>
  select(-measuredat2,-iv_mode) |>
  distinct() |>
  filter(map > 0) |>
  filter(dbp >0, dbp < 1000) |> # filtering negative map
  filter(sbp > 0, sbp < 1000)
  
df1 |>
  pivot_longer(cols = c(a2_resp_rate:a2_ht,pfr,vfd7,vfd28), names_to = "param", values_to = "value") |> 
  ggplot(aes(x=mp,y=value)) +
  geom_point(alpha=0.1) +
  facet_wrap(.~param, scales = "free") +
  theme_minimal()

df2 
df1 <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23/dfping3.csv") |>
  rename(mp=MP,vfd28=VFDs) |>
  filter(tube >= 2*24) 
admin <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23/outputs/admission_reason_v2.csv")


dotgraph <- "digraph { map -> vfd28; sbp -> vfd28; dbp -> vfd28; plt -> vfd28; a2_heart_rate -> vfd28; a2_temperature -> vfd28; a2_mean_abp -> vfd28; a2_pH -> vfd28; a2_sodium -> vfd28; a2_potassium -> vfd28; a2_creatinine_mg_per_dl -> vfd28; a2_ht -> vfd28; mp -> vfd28; a2_resp_rate -> mp; tidal -> mp; minute_vol -> mp; peep -> mp; insp_p -> mp }"

dotuniq <- str_extract_all(dotgraph, "(?<= ).*?(?= |;)") |> 
  unlist() |> 
  unique() |>
  str_replace(pattern = "->|\\{", replacement = "***")

dag_col <- c()

for (i in dotuniq) { 
  if (i=="***") {} else {dag_col <- append(dag_col, i)}}

df1_me <- df |>
  # slice_sample(prop = 0.5) |>
  mutate(mp = 0.098 * tidal/1000 * a2_resp_rate * peep + insp_p,
         pfr = PaO2/FiO2*100) |> 
  left_join(admin |>
              select(admissionid, starts_with("is_"), death2)) |>
  group_by(admissionid) |>
  mutate(across(.cols = a2_resp_rate:pfr, .fn = ~median(.,na.rm = T))) |>
  mutate(total_vent = sum(is_iv, na.rm = T)/24,
         vent7 = 7 - total_vent,
         vent28 = 28 - total_vent,
         dead = case_when(
           !is.na(death2) ~ 1,
           TRUE ~ 0
         ),
         vfd7 = case_when(
           dead == 0 & vent7 > 0 ~ vent7,
           TRUE ~ 0
         ),
         vfd28 = case_when(
           dead == 0 & vent28 > 0 ~ vent28,
           TRUE ~ 0)) |>
  select(-death2, all_of(dag_col), -is_iv, -bilirubin) |> 
  drop_na() |>
    distinct(admissionid, vfd28, .keep_all = T) |>
  filter(total_vent >= 2)

  
df1 |>  
  select(admissionid, measuredat2, mp, starts_with("is_")) |>
  pivot_longer(cols = starts_with("is_"), names_to = "category", values_to = "value") |>
  filter(measuredat2 >= 0) |> 
  filter(measuredat2 <= 24*28) |>
  filter(mp < 500) |>
  filter(category != "is_iv") |>
  filter(category != "is_invasive_ventilation") |>
  mutate(category = case_when(
    value == TRUE ~ category,
    TRUE ~ NA_character_
  )) |> drop_na(category) |> 
  ggplot(aes(x=measuredat2,y=mp,color=category)) +
  geom_line(alpha=0.1) +
  facet_wrap(.~category) +
  theme_minimal() +
  theme(legend.position = "none")

df1 |>
  filter(is_surgical==F,is_medical==F) |> 
  ungroup() |>
  select(admissionid, starts_with("is_")) |>
  pivot_longer(cols = starts_with("is_"), names_to = "category", values_to = "value") |>
  group_by(category) |>
  summarize(n = sum(value)) |>
  arrange(desc(n))


```

# splitting hr , mp, ate of pfr
```{r}
df <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23/outputs/curr_final_v6.csv")
admin <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23/outputs/admission_reason_v2.csv")


# filter out na
dfall <- df |>
  left_join(admin |> select(admissionid, starts_with("is_")), by = "admissionid") |>
  group_by(admissionid) |>
  filter(is_iv == T) |>
  mutate(hr = row_number()) |> 
  # filter(hr >= 48) |>
  select(-bilirubin,-gcs) |>
  # ungroup() |>
  # filter(!is.na(PaO2)) |>
  group_by(admissionid) |>
  fill(everything(), .direction = "down") |>
  fill(everything(), .direction = "up") |>
  mutate(mp = 0.098 * tidal/1000 * a2_resp_rate * (peep + insp_p),
         sfr = spo2/FiO2*100) 

dw <- import("dowhy")
sk <- import("sklearn")
econ <- import("econml")
xgb <- import("xgboost")

dotgraph <- "digraph { map -> vfd28; sbp -> vfd28; dbp -> vfd28; plt -> vfd28; a2_heart_rate -> vfd28; a2_temperature -> vfd28; a2_mean_abp -> vfd28; a2_pH -> vfd28; a2_sodium -> vfd28; a2_potassium -> vfd28; a2_creatinine_mg_per_dl -> vfd28; a2_ht -> vfd28; mp -> vfd28; a2_resp_rate -> mp; tidal -> mp; minute_vol -> mp; peep -> mp; insp_p -> mp }"

dotgraph <- str_replace_all(dotgraph, pattern = "vfd28", replacement = "pfr")


category <- dfall |>
  ungroup() |>
  select(starts_with("is_")) |>
  # select(-is_invasive_ventilation) |>
  colnames()

df_ate <- tibble(n=as.numeric(),threshold=as.numeric(),hr=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())

# category <- c("is_surgical","is_medical")

category |> unique()

tic()
for (col in category) {
  
df_col <- dfall |>
  ungroup() |>
  filter(!!sym(col)==T) 

for (j in c(1:48)) {
df_hr <- df_col |>
  filter(hr == j)

for (i in seq(5,40,1)) {
  df_mp <- df_hr |>
    mutate(mp = case_when(
      mp >= i ~ 1,
      TRUE ~ 0
    )) |>
    drop_na()
  
  n <- nrow(df_mp |> filter(mp==1))

model <- dw$CausalModel(data = df_mp,treatment = "mp",outcome = "sfr",graph = dotgraph)

id_effect <- model$identify_effect(proceed_when_unidentifiable = T)

# estimate <- model$estimate_effect(identified_estimand = id_effect, control_value = 0, treatment_value = 1, method_name = "backdoor.linear_regression", confidence_intervals = T)
# py_dict <- dict(init_params = dict(model_y = gbr,
#                                    model_t = gbc,
#                                    discrete_treatment = T,
#                                    random_state = as.integer(100),
#                                    model_final = debiasedlasso),
#                 fit_params = dict())

estimate <- model$estimate_effect(identified_estimand = id_effect, method_name = "backdoor.linear_regression", confidence_intervals = T)

ate <- estimate$value
ate_ci <- estimate$get_confidence_intervals()

df_ate <- df_ate |>
  add_row(n=n,threshold=i,hr=j,category=col,ate=ate,lower_ci=ate_ci[[1]],upper_ci=ate_ci[[2]])

print(paste0("n: ",n," cat: ",col," hr: ", j, " threshold: ",i, " ate:",ate, " [",ate_ci[[1]],"-",ate_ci[[2]],"]"))
}
}
}
toc()

write_csv(df_ate, "df_ate_newmp_72h_50mp_medical_surgical.csv")

# write_csv(df_ate, file = "df_ate_hr_mp3.csv")
df_ate <- df_temp
write_csv(df_ate, "df_ate_newmp_96h_50mp_med_surg_noimpute.csv")
write_csv(df_ate, "df_ate_newmp_96h_50mp_sfr_med_surg_impute.csv")

dftest <- read_csv("df_ate_hr_mp2.csv")
dftest |> group_by(category) |> summarise(n = n())
# install.packages("plotly")
library(plotly)

df_ate <- read_csv("df_ate_48h_40mp_sfr_cohort_facet.csv")

fig <- plot_ly(df_ate |> filter(category == "is_shock"), x=~hr, z=~ate, y=~threshold, alpha=0.2, type = "mesh3d", colorscale = c("lightpink","red","purple")) 

fig

med <- df_ate |>
  # filter(category == "is_medical") |>
  ggplot(aes(x=hr,y=threshold,z=ate)) +
  geom_contour() +
  theme_minimal() +
  facet_wrap(.~category)
  ggtitle("Medical")
  
  med

surg <- df_ate |>
  filter(category == "is_surgical") |>
  ggplot(aes(x=hr,y=threshold,fill=ate)) +
  geom_tile() +
  theme_minimal() +
  ggtitle("Surgical")

library(ggpubr)

ggarrange(med,surg)
```

# parallelism
```{r}
# library(doParallel)

# parallel train model ~ 0.76s per model, non-parallel 0.98s per model
library(doSNOW)


cl <- makeSOCKcluster(10, outfile = "")
registerDoSNOW(cl)


dfall <- df |>
  left_join(admin |> select(admissionid, starts_with("is_")), by = "admissionid") |>
  group_by(admissionid) |>
  filter(is_iv == T) |>
  mutate(hr = row_number()) |> 
  select(-bilirubin,-gcs) |>
  fill(everything(), .direction = "down") |>
  fill(everything(), .direction = "up") |>
  mutate(mp = 0.098 * tidal/1000 * a2_resp_rate * (peep + insp_p),
         pfr = PaO2/FiO2*100) 

dw <- import("dowhy")
sk <- import("sklearn")
econ <- import("econml")
xgb <- import("xgboost")

dotgraph <- "digraph { map -> vfd28; sbp -> vfd28; dbp -> vfd28; plt -> vfd28; a2_heart_rate -> vfd28; a2_temperature -> vfd28; a2_mean_abp -> vfd28; a2_pH -> vfd28; a2_sodium -> vfd28; a2_potassium -> vfd28; a2_creatinine_mg_per_dl -> vfd28; a2_ht -> vfd28; mp -> vfd28; a2_resp_rate -> mp; tidal -> mp; minute_vol -> mp; peep -> mp; insp_p -> mp }"

dotgraph <- str_replace_all(dotgraph, pattern = "vfd28", replacement = "sfr")

category <- dfall |>
  ungroup() |>
  select(starts_with("is_")) |>
  # select(-is_invasive_ventilation) |>
  colnames()

df_ate <- tibble(n=as.numeric(),threshold=as.numeric(),hr=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())

# df_ate_init <- tibble(n=as.numeric(),threshold=as.numeric(),hr=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())

category <- c("is_surgical","is_medical")



category |> unique()
tic()
df_temp <- foreach(col = category, .combine = "rbind") %dopar% {
   library(dplyr)
  library(tidyr)
  df_ate <- tibble(n=as.numeric(),threshold=as.numeric(),hr=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())
 
 
  dw <- reticulate::import("dowhy")
   
df_col <- dfall |>
  ungroup() |>
  filter(!!sym(col)==T) 

for (j in c(1:48)) {
df_hr <- df_col |>
  filter(hr == j)

for (i in seq(5,40,1)) {
  # library(tidyverse)
  # df_ate <- tibble(n=as.numeric(),threshold=as.numeric(),hr=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())
  
   # dw <- reticulate::import("dowhy")
  # library(reticulate)
 
  
  df_mp <- df_hr |>
    mutate(mp = case_when(
      mp >= i ~ 1,
      TRUE ~ 0
    )) |>
    drop_na()
  
  n <- nrow(df_mp |> filter(mp==1))

model <- dw$CausalModel(data = df_mp,treatment = "mp",outcome = "sfr",graph = dotgraph)

id_effect <- model$identify_effect(proceed_when_unidentifiable = T)

# estimate <- model$estimate_effect(identified_estimand = id_effect, control_value = 0, treatment_value = 1, method_name = "backdoor.linear_regression", confidence_intervals = T)
# py_dict <- dict(init_params = dict(model_y = gbr,
#                                    model_t = gbc,
#                                    discrete_treatment = T,
#                                    random_state = as.integer(100),
#                                    model_final = debiasedlasso),
#                 fit_params = dict())

estimate <- model$estimate_effect(identified_estimand = id_effect, method_name = "backdoor.linear_regression", confidence_intervals = T)

ate <- estimate$value
ate_ci <- estimate$get_confidence_intervals()

# df_ate <- df_ate |>
#   add_row(n=n,threshold=i,hr=j,category=col,ate=ate,lower_ci=ate_ci[[1]],upper_ci=ate_ci[[2]])

df_return <- tibble(n=n,threshold=i,hr=j,category=col,ate=ate,lower_ci=ate_ci[[1]],upper_ci=ate_ci[[2]])


print(paste0("n: ",n," cat: ",col," hr: ", j, " threshold: ",i, " ate:",ate, " [",ate_ci[[1]],"-",ate_ci[[2]],"]"))

df_ate <- rbind(df_ate,df_return)


}
# df_ate <- rbind(df_ate,df_temp)
} 
return(df_ate) 
} 
toc()

df_ate <- df_temp

write_csv(df_ate, "df_ate_48h_40mp_sfr_cohort_facet.csv")


lasttimepar <- 146.164
# df_ate <- rbind(df_ate,df_return)
stopCluster(cl)
# stopCluster(cl)

```




# looking at last mp prior to vent liberation, no death, no trach
```{r}
# abandoned death because i don't know if death unit
dfdeath <- df |>
  left_join(admin |> select(admissionid, starts_with("is_")), by = "admissionid") |>
  group_by(admissionid) |>
  filter(is_iv == T) |>
  mutate(hr = row_number()) |> 
  select(-bilirubin,-gcs) |>
  fill(everything(), .direction = "down") |>
  fill(everything(), .direction = "up") |>
  mutate(mp = 0.098 * tidal/1000 * a2_resp_rate * peep + insp_p,
         pfr = PaO2/FiO2*100) |>
  left_join(admin) 

# trach
trach <- df |>
  filter(tracheostoma == T) |>
  distinct(admissionid) |>
  pull()

dead <- admin |>
  filter(!is.na(death2)) |>
  select(admissionid) |>
  pull()

dfmppriortoextubate <- dfall |>
  filter(!admissionid %in% dead) |>
  filter(!admissionid %in% trach) |>
  mutate(lastrow = n()) |>
  filter(hr == lastrow) 

sumdf <- summary(dfmppriortoextubate |> ungroup() |> select(mp))

dfall |>
  filter(!admissionid %in% dead) |>
  filter(!admissionid %in% trach) |>
  mutate(lastrow = n()) |>
  filter(hr == lastrow) |>
  # select(mp) |> view()
  ggplot(aes(x=mp)) +
  geom_histogram() +
  annotate(geom = "text", x = 90, y = 1700, label = paste0(str_c(sumdf, collapse = "\n"))) +
  ggtitle("Last MP prior to extubation - no death, no trach")


```



# dowhy
```{r}
dw <- import("dowhy")
sk <- import("sklearn")
econ <- import("econml")
xgb <- import("xgboost")

gbr <- sk$ensemble$GradientBoostingRegressor()
gbc <- sk$ensemble$GradientBoostingClassifier()
debiasedlasso <- econ$sklearn_extensions$linear_model$DebiasedLasso()
xgbr <- xgb$XGBRegressor()
k
dotgraph <- "digraph { map -> vfd28; sbp -> vfd28; dbp -> vfd28; plt -> vfd28; a2_heart_rate -> vfd28; a2_temperature -> vfd28; a2_mean_abp -> vfd28; a2_pH -> vfd28; a2_sodium -> vfd28; a2_potassium -> vfd28; a2_creatinine_mg_per_dl -> vfd28; a2_ht -> vfd28; mp -> vfd28; a2_resp_rate -> mp; tidal -> mp; minute_vol -> mp; peep -> mp; insp_p -> mp }"

category <- df1 |>
  ungroup() |>
  select(starts_with("is_")) |>
  # select(-is_invasive_ventilation) |>
  colnames()
# category <- "is_gastrointestinal_surgery"

df_ate <- tibble(n=as.numeric(),threshold=as.numeric(),category=as.character(),ate=as.numeric(),lower_ci=as.numeric(),upper_ci=as.numeric())

for (col in category) {

  df2 <- df1 |>
    # filter(total_vent >= 2) |>
    # filter(total_vent <= 7) |>
    filter(!!sym(col)==1)
  
  seqi <- seq(5,40,1)
  for (i in seqi) {
    
    df3 <- df2 |>
      mutate(mp = case_when(
        mp >= i ~ 1,
        TRUE ~ 0
      ))
    
    ntotal <- df2 |>
      mutate(mp = case_when(
        mp >= seqi[1] ~ 1,
        TRUE ~ 0
      ))
    
    n <- nrow(df3 |> filter(mp==1))
    
    if (n<10 | nrow(ntotal)==n | n==(nrow(ntotal)-1)) { next } else {
    
model <- dw$CausalModel(data = df3,treatment = "mp",outcome = "vfd28",graph = dotgraph)

id_effect <- model$identify_effect(proceed_when_unidentifiable = T)

# estimate <- model$estimate_effect(identified_estimand = id_effect, control_value = 0, treatment_value = 1, method_name = "backdoor.linear_regression", confidence_intervals = T)
py_dict <- dict(init_params = dict(model_y = gbr,
                                   model_t = gbc,
                                   discrete_treatment = T,
                                   random_state = as.integer(100),
                                   model_final = debiasedlasso),
                fit_params = dict())

estimate <- model$estimate_effect(identified_estimand = id_effect, method_name = "backdoor.econml.dml.NonParamDML", confidence_intervals = T, method_params = py_dict)


ate <- estimate$value
# ate_ci <- estimate$get_confidence_intervals()
ate_ci <- list(quantile(estimate$cate_estimates, 0.025),quantile(estimate$cate_estimates, 0.975))

df_ate <- df_ate |>
  add_row(n=n,threshold=i,category=col,ate=ate,lower_ci=ate_ci[[1]],upper_ci=ate_ci[[2]])

print(paste0("n: ",n," cat: ",col," threshold: ",i, " ate:",ate, " [",ate_ci[[1]],"-",ate_ci[[2]],"]"))
}}}

write_csv(df_ate, "ping_ateci_mean_cf.csv")

df_ate |>
  # filter(ate <10) |>
  # filter(ate >-10) |>
  ggplot(aes(x=threshold,y=ate)) +
  # geom_point() +
  # geom_text_repel(aes(label = paste0("T1_n: ",n)), ) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red", alpha = 0.2) +
  # geom_smooth(se = F) +
  geom_ribbon(aes(x=threshold,ymin=lower_ci,ymax=upper_ci),alpha=0.2) +
  facet_wrap(.~category, scales = "free_y") +
  theme_minimal() +
  # ylim(-10,10) +
  ggtitle("ATE (VFD28) vs. MP threshold (mean) - NonParamD") +
  coord_cartesian(ylim = c(-10,10))

# write_csv(df_ate, file = "ping_df_ate_meanci_np.csv")

dflr <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/An R Data/ping_ate_meanci_vfd.csv") |> mutate(method="linear_regression")
dfcf <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/An R Data/ping_df_ate_meanci_cf.csv") |> mutate(method = "causal_forest")
dfnp <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/An R Data/ping_df_ate_meanci_np.csv") |> mutate(method = "non_param")

dfcom <- rbind(dflr,dfcf,dfnp)

dfcom |>
  ggplot(aes(x=threshold,y=ate,fill=method)) +
  geom_line(aes(color=method), alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", alpha = 0.2) +
  # geom_smooth(se = F) +
  geom_ribbon(aes(x=threshold,ymin=lower_ci,ymax=upper_ci),alpha=0.2) +
  facet_wrap(.~category, scales = "free_y") +
  theme_minimal() +
  # ylim(-15,15) +
  ggtitle("ATE (VFD28) vs. MP threshold (mean)", subtitle = "Red = Causal Forest; Green = Linear Regression; Blue = Non-Paramateric") +
  coord_cartesian(ylim = c(-15,15)) +  
  theme(legend.position = "None")

# visualizing both surgical and medical
df_ate |>
  filter(category %in% c("is_surgical","is_medical")) |>
  ggplot(aes(x=threshold,y=ate,fill=category)) +
  # geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  geom_ribbon(aes(x=threshold,ymin=lower_ci,ymax=upper_ci),alpha=0.2) +
  theme_minimal() +
  ylim(-10,10) +
  xlim(5,30)
  coord_cartesian(ylim = c(-10,10))
  

model$do(x = df1, identified_estimand = id_effect, method_name = "backdoor.linear.regression")

# econml
ec <- import("econml")
ec$dml$CausalForestDML

dfping <- read_csv("/Users/kenkoonwong/Google Drive/My Drive/datathon23 copy/pingdf.csv") |> 
  select(-tube)

dotgraph <- "digraph { map -> VFDs; sbp -> VFDs; dbp -> VFDs; plt -> VFDs; a2_heart_rate -> VFDs; a2_temperature -> VFDs; a2_mean_abp -> VFDs; a2_pH -> VFDs; a2_sodium -> VFDs; a2_potassium -> VFDs; a2_creatinine_mg_per_dl -> VFDs; a2_ht -> VFDs; MP -> VFDs; a2_resp_rate -> MP; tidal -> MP; minute_vol -> MP; peep -> MP; insp_p -> MP }"
model <- dw$CausalModel(data = dfping,treatment = "MP",outcome = "VFDs",graph = dotgraph)

id_effect <- model$identify_effect(proceed_when_unidentifiable = T)

# param <- list("init_params","fit")
# pyparam <- r_to_py(param)

sk <- import("sklearn")

gbr <- sk$ensemble$GradientBoostingRegressor()
np <- import("numpy")

# from sklearn.ensemble import GradientBoostingRegressor
# from econml.dml import CausalForestDML

# Create the Python dictionary
py_dict <- dict(init_params = dict(model_y = gbr,
                                   model_t = gbr,
                                   discrete_treatment = F,
                                   random_state = as.integer(100)),
                                   # model_final = "auto"),
                fit_params = dict())

py_dict <- dict(init_params = dict(),fit_params = dict())

# Convert the Python dictionary to an R list
r_list <- py_to_r(py_dict)

# Print the converted R list
print(r_list)

df_py <- r_to_py(df1)

library(recipes)

df_re <- recipe(~.,data = df1) |>
  step_normalize(all_numeric()) |>
  prep() |>
  juice()

model <- dw$CausalModel(data = df1,treatment = "mp",outcome = "vfd28",graph = dotgraph)

id_effect <- model$identify_effect(proceed_when_unidentifiable = T)

rm(estimateec)

estimateec <- model$estimate_effect(identified_estimand = id_effect, method_name = "backdoor.econml.dml.CausalForestDML", control_value = as.integer(20), treatment_value = as.integer(30), method_params = py_dict, confidence_intervals = T)

# esci <- estimateec$get_confidence_intervals(confidence_level = 0.95, method = "bootstrap")

builtins <- import_builtins()
builtins$print(estimateec)
# estimateec$get_confidence_intervals(confidence_level = 0.95,method = "bootstrap")
est <- estimateec$cate_estimates
quantile(est, probs = 0.025)
quantile(est, probs = 0.975)
estimateec$interpret()




library(DiagrammeR)

# Create the node data frame
node_df <- create_node_df(n = 19, nodes = c("map", "vfd28", "sbp", "dbp", "plt", "a2_heart_rate",
                                    "a2_temperature", "a2_mean_abp", "a2_pH", "a2_sodium",
                                    "a2_potassium", "a2_creatinine_mg_per_dl", "a2_ht",
                                    "mp", "a2_resp_rate", "tidal", "peep",
                                    "insp_p","peak"), label = c("map", "vfd28", "sbp", "dbp", "plt", "a2_heart_rate","a2_temperature", "a2_mean_abp", "a2_pH", "a2_sodium",
                                    "a2_potassium", "a2_creatinine_mg_per_dl", "a2_ht",
                                    "mp", "a2_resp_rate", "tidal", "peep",
                                    "insp_p","peak")
                          )

# Create the edge data frame
edge_df <- create_edge_df(from = c(1,3:19,16,17),
                          to = c(rep(2,13),14,19,19,19,14,14,14))

# Create the graph using the node and edge data frames
graph <- create_graph(nodes_df = node_df, edges_df = edge_df) 

# Plot the graph
render_graph(graph)



```

```{python}
from dowhy import CausalModel
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LassoCV
from sklearn.ensemble import GradientBoostingRegressor
from econml.dml import CausalForestDML
from econml.sklearn_extensions.linear_model import DebiasedLasso
import numpy as np
import pandas as pd
import dowhy
import econml
# 
dowhy.__version__
econml.__version__

df = r.df1
t = ['mp']
y = ['vfd28']
# 
# df2 = pd.DataFrame(df)
# 
model = CausalModel(
      data=df,
      treatment=t,
      outcome=y,
      graph=r.dotgraph)

  identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
  # identifier_name = identified_estimand.estimands
  # 
  #   causal_estimate = model.estimate_effect(identified_estimand,
  #     method_name="backdoor.linear_regression",
  #     test_significance=True,
  #   confidence_intervals =True)
  # 
  #   print(causal_estimate)
    
del dml_estimate
    
dml_estimate = model.estimate_effect(identified_estimand=identified_estimand, method_name='backdoor.econml.dml.CausalForestDML', #"backdoor.econml.dml.DML",
control_value=0, treatment_value=1,
                                        confidence_intervals=True,
                                        method_params={
                        "init_params":{'model_y': GradientBoostingRegressor(),
                                      'model_t':  GradientBoostingRegressor()
                                    #  'model_final': DebiasedLasso(), # 'auto',
                                      #'featurizer':PolynomialFeatures(degree=1, include_bias=True)
                                      },
                        "fit_params":{},
                        'discrete_treatment' : False,
                        }
                                            )
dml_estimate.interpret()
print(dml_estimate)

dml_estimate
```

```{python}
import numpy as np
import pandas as pd
import logging

import dowhy
from dowhy import CausalModel
import dowhy.datasets

import econml
import warnings
warnings.filterwarnings('ignore')

BETA = 10

data = dowhy.datasets.linear_dataset(BETA, num_common_causes=5, num_samples=100,
                                    num_instruments=3, num_effect_modifiers=2,
                                     num_treatments=1,
                                    treatment_is_binary=False,
                                    num_discrete_common_causes=2,
                                    num_discrete_effect_modifiers=0,
                                    one_hot_encode=False)
df=data['df']
data['treatment_name']
data['instrument_names']
data['outcome_name']
data['common_causes_names']
data['effect_modifier_names']

data['gml_graph']
plot(data['gml_graph'])
print(df.head())
print("True causal estimate is", data["ate"])

model = CausalModel(data=data["df"], 
                    treatment=data["treatment_name"], outcome=data["outcome_name"], 
                    graph=data["gml_graph"])
                    
model.view_model()
from IPython.display import Image, display
display(Image(filename="causal_model.png"))

identified_estimand= model.identify_effect(proceed_when_unidentifiable=True)
print(identified_estimand)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LassoCV
from sklearn.ensemble import GradientBoostingRegressor
dml_estimate = model.estimate_effect(identified_estimand, method_name="backdoor.econml.dml.CausalForestDML",
                                     # control_value = 0,
                                     # treatment_value = 1,
                                 # target_units = lambda df: df["X0"]>1,  # condition used for CATE
                                confidence_intervals=True,
                                method_params={"init_params":{'model_y':GradientBoostingRegressor(),
                                                              'model_t': GradientBoostingRegressor()},
                                                              # "model_final":LassoCV(fit_intercept=False), 
                                                              # 'featurizer':PolynomialFeatures(degree=1, include_bias=False)},
                                               "fit_params":{}})
print(dml_estimate)
```


```{python}
from econml.panel.dml import DynamicDML

np.random.seed(123)

n_panels = 100  # number of panels
n_periods = 3  # number of time periods per panel
n = n_panels * n_periods
groups = np.repeat(a=np.arange(n_panels), repeats=n_periods, axis=0)
X = np.random.normal(size=(n, 3))
T = np.random.normal(size=(n, 2))
y = np.random.normal(size=(n, ))
W = np.random.normal(size=(n,2))

est = DynamicDML()
est.fit(y, T, X=X, W=W, groups=groups, inference="auto")

est.ate(X[1:n],T0=0,T1=1)
est.ate_inference(X[1:n],T0=0,T1=1)
est.const_marginal_ate(X[1:n])
est.summary()
```

# econml testing dynamic
```{python}
# Copyright (c) PyWhy contributors. All rights reserved.
# Licensed under the MIT License.
import abc
import numpy as np
from econml.utilities import cross_product
from statsmodels.tools.tools import add_constant
try:
    import matplotlib
    import matplotlib.pyplot as plt
except ImportError as exn:
    from .utilities import MissingModule

    # make any access to matplotlib or plt throw an exception
    matplotlib = plt = MissingModule("matplotlib is no longer a dependency of the main econml package; "
                                     "install econml[plt] or econml[all] to require it, or install matplotlib "
                                     "separately, to use the tree interpreters", exn)


class _BaseDynamicPanelDGP:

    def __init__(self, n_periods, n_treatments, n_x):
        self.n_periods = n_periods
        self.n_treatments = n_treatments
        self.n_x = n_x
        return

    @abc.abstractmethod
    def create_instance(self, *args, **kwargs):
        pass

    @abc.abstractmethod
    def _gen_data_with_policy(self, n_units, policy_gen, random_seed=123):
        pass

    def static_policy_data(self, n_units, tau, random_seed=123):
        def policy_gen(Tpre, X, period):
            return tau[period]
        return self._gen_data_with_policy(n_units, policy_gen, random_seed=random_seed)

    def adaptive_policy_data(self, n_units, policy_gen, random_seed=123):
        return self._gen_data_with_policy(n_units, policy_gen, random_seed=random_seed)

    def static_policy_effect(self, tau, mc_samples=1000):
        Y_tau, _, _, _ = self.static_policy_data(mc_samples, tau)
        Y_zero, _, _, _ = self.static_policy_data(
            mc_samples, np.zeros((self.n_periods, self.n_treatments)))
        return np.mean(Y_tau[np.arange(Y_tau.shape[0]) % self.n_periods == self.n_periods - 1]) - \
            np.mean(Y_zero[np.arange(Y_zero.shape[0]) %
                           self.n_periods == self.n_periods - 1])

    def adaptive_policy_effect(self, policy_gen, mc_samples=1000):
        Y_tau, _, _, _ = self.adaptive_policy_data(mc_samples, policy_gen)
        Y_zero, _, _, _ = self.static_policy_data(
            mc_samples, np.zeros((self.n_periods, self.n_treatments)))
        return np.mean(Y_tau[np.arange(Y_tau.shape[0]) % self.n_periods == self.n_periods - 1]) - \
            np.mean(Y_zero[np.arange(Y_zero.shape[0]) %
                           self.n_periods == self.n_periods - 1])


class DynamicPanelDGP(_BaseDynamicPanelDGP):

    def __init__(self, n_periods, n_treatments, n_x):
        super().__init__(n_periods, n_treatments, n_x)

    def create_instance(self, s_x, sigma_x=.8, sigma_y=.1, conf_str=5, hetero_strength=.5, hetero_inds=None,
                        autoreg=.25, state_effect=.25, random_seed=123):
        np.random.seed(random_seed)
        self.s_x = s_x
        self.conf_str = conf_str
        self.sigma_x = sigma_x
        self.sigma_y = sigma_y
        self.hetero_inds = hetero_inds.astype(
            int) if hetero_inds is not None else hetero_inds
        self.endo_inds = np.setdiff1d(
            np.arange(self.n_x), hetero_inds).astype(int)
        # The first s_x state variables are confounders. The final s_x variables are exogenous and can create
        # heterogeneity
        self.Alpha = np.random.uniform(-1, 1,
                                       size=(self.n_x, self.n_treatments))
        self.Alpha /= np.linalg.norm(self.Alpha, axis=1, ord=1, keepdims=True)
        self.Alpha *= state_effect
        if self.hetero_inds is not None:
            self.Alpha[self.hetero_inds] = 0

        self.Beta = np.zeros((self.n_x, self.n_x))
        for t in range(self.n_x):
            self.Beta[t, :] = autoreg * np.roll(np.random.uniform(low=4.0**(-np.arange(
                0, self.n_x)), high=4.0**(-np.arange(1, self.n_x + 1))), t)
        if self.hetero_inds is not None:
            self.Beta[np.ix_(self.endo_inds, self.hetero_inds)] = 0
            self.Beta[np.ix_(self.hetero_inds, self.endo_inds)] = 0

        self.epsilon = np.random.uniform(-1, 1, size=self.n_treatments)
        self.zeta = np.zeros(self.n_x)
        self.zeta[:self.s_x] = self.conf_str / self.s_x

        self.y_hetero_effect = np.zeros(self.n_x)
        self.x_hetero_effect = np.zeros(self.n_x)
        if self.hetero_inds is not None:
            self.y_hetero_effect[self.hetero_inds] = np.random.uniform(.5 * hetero_strength, 1.5 * hetero_strength) / \
                len(self.hetero_inds)
            self.x_hetero_effect[self.hetero_inds] = np.random.uniform(.5 * hetero_strength, 1.5 * hetero_strength) / \
                len(self.hetero_inds)

        self.true_effect = np.zeros((self.n_periods, self.n_treatments))
        # Invert indices to match latest API
        self.true_effect[self.n_periods - 1] = self.epsilon
        for t in np.arange(self.n_periods - 2, -1, -1):
            self.true_effect[t, :] = (self.zeta.reshape(
                1, -1) @ np.linalg.matrix_power(self.Beta, (self.n_periods - 1 - t) - 1) @ self.Alpha)

        self.true_hetero_effect = np.zeros(
            (self.n_periods, (self.n_x + 1) * self.n_treatments))
        self.true_hetero_effect[self.n_periods - 1, :] = cross_product(
            add_constant(self.y_hetero_effect.reshape(1, -1), has_constant='add'),
            self.epsilon.reshape(1, -1))
        for t in np.arange(self.n_periods - 2, -1, -1):
            # Invert indices to match latest API
            self.true_hetero_effect[t, :] = cross_product(
                add_constant(self.x_hetero_effect.reshape(1, -1), has_constant='add'),
                self.zeta.reshape(1, -1) @ np.linalg.matrix_power(
                    self.Beta, (self.n_periods - 1 - t) - 1) @ self.Alpha)
        return self

    def hetero_effect_fn(self, t, x):
        if t == self.n_periods - 1:
            return (np.dot(self.y_hetero_effect, x.flatten()) + 1) * self.epsilon
        else:
            return (np.dot(self.x_hetero_effect, x.flatten()) + 1) *\
                (self.zeta.reshape(1, -1) @ np.linalg.matrix_power(self.Beta, (self.n_periods - 1 - t) - 1)
                    @ self.Alpha).flatten()

    def _gen_data_with_policy(self, n_units, policy_gen, random_seed=123):
        np.random.seed(random_seed)
        Y = np.zeros(n_units * self.n_periods)
        T = np.zeros((n_units * self.n_periods, self.n_treatments))
        X = np.zeros((n_units * self.n_periods, self.n_x))
        groups = np.zeros(n_units * self.n_periods)
        for t in range(n_units * self.n_periods):
            period = t % self.n_periods
            if period == 0:
                X[t] = np.random.normal(0, self.sigma_x, size=self.n_x)
                const_x0 = X[t][self.hetero_inds]
                T[t] = policy_gen(np.zeros(self.n_treatments), X[t], period)
            else:
                X[t] = (np.dot(self.x_hetero_effect, X[t - 1]) + 1) * np.dot(self.Alpha, T[t - 1]) + \
                    np.dot(self.Beta, X[t - 1]) + \
                    np.random.normal(0, self.sigma_x, size=self.n_x)
                # The feature for heterogeneity stays constant
                X_t = X[t].copy()
                X_t[self.hetero_inds] = const_x0
                T[t] = policy_gen(T[t - 1], X[t], period)
            Y[t] = (np.dot(self.y_hetero_effect, X_t if period != 0 else X[t]) + 1) * np.dot(self.epsilon, T[t]) + \
                np.dot(X[t], self.zeta) + \
                np.random.normal(0, self.sigma_y)
            groups[t] = t // self.n_periods

        return Y, T, X[:, self.hetero_inds] if (self.hetero_inds is not None) else None, X[:, self.endo_inds], groups

    def observational_data(self, n_units, gamma=0, s_t=1, sigma_t=0.5, random_seed=123):
        """Generate observational data with some observational treatment policy parameters.

        Parameters
        ----------
        n_units : how many units to observe
        gamma : what is the degree of auto-correlation of the treatments across periods
        s_t : sparsity of treatment policy; how many states does it depend on
        sigma_t : what is the std of the exploration/randomness in the treatment
        """
        Delta = np.zeros((self.n_treatments, self.n_x))
        Delta[:, :s_t] = self.conf_str / s_t

        def policy_gen(Tpre, X, period):
            return gamma * Tpre + (1 - gamma) * np.dot(Delta, X) + \
                np.random.normal(0, sigma_t, size=self.n_treatments)
        return self._gen_data_with_policy(n_units, policy_gen, random_seed=random_seed)


# Auxiliary function for adding xticks and vertical lines when plotting results
# for dynamic dml vs ground truth parameters.
def add_vlines(n_periods, n_treatments, hetero_inds):
    locs, labels = plt.xticks([], [])
    locs += [- .501 + (len(hetero_inds) + 1) / 2]
    labels += ["\n\n$\\tau_{{{}}}$".format(0)]
    locs += [qx for qx in np.arange(len(hetero_inds) + 1)]
    labels += ["$1$"] + ["$x_{{{}}}$".format(qx) for qx in hetero_inds]
    for q in np.arange(1, n_treatments):
        plt.axvline(x=q * (len(hetero_inds) + 1) - .5,
                    linestyle='--', color='red', alpha=.2)
        locs += [q * (len(hetero_inds) + 1) - .501 + (len(hetero_inds) + 1) / 2]
        labels += ["\n\n$\\tau_{{{}}}$".format(q)]
        locs += [(q * (len(hetero_inds) + 1) + qx)
                 for qx in np.arange(len(hetero_inds) + 1)]
        labels += ["$1$"] + ["$x_{{{}}}$".format(qx) for qx in hetero_inds]
    locs += [- .501 + (len(hetero_inds) + 1) * n_treatments / 2]
    labels += ["\n\n\n\n$\\theta_{{{}}}$".format(0)]
    for t in np.arange(1, n_periods):
        plt.axvline(x=t * (len(hetero_inds) + 1) *
                    n_treatments - .5, linestyle='-', alpha=.6)
        locs += [t * (len(hetero_inds) + 1) * n_treatments - .501 +
                 (len(hetero_inds) + 1) * n_treatments / 2]
        labels += ["\n\n\n\n$\\theta_{{{}}}$".format(t)]
        locs += [t * (len(hetero_inds) + 1) *
                 n_treatments - .501 + (len(hetero_inds) + 1) / 2]
        labels += ["\n\n$\\tau_{{{}}}$".format(0)]
        locs += [t * (len(hetero_inds) + 1) * n_treatments +
                 qx for qx in np.arange(len(hetero_inds) + 1)]
        labels += ["$1$"] + ["$x_{{{}}}$".format(qx) for qx in hetero_inds]
        for q in np.arange(1, n_treatments):
            plt.axvline(x=t * (len(hetero_inds) + 1) * n_treatments + q * (len(hetero_inds) + 1) - .5,
                        linestyle='--', color='red', alpha=.2)
            locs += [t * (len(hetero_inds) + 1) * n_treatments + q *
                     (len(hetero_inds) + 1) - .501 + (len(hetero_inds) + 1) / 2]
            labels += ["\n\n$\\tau_{{{}}}$".format(q)]
            locs += [t * (len(hetero_inds) + 1) * n_treatments + (q * (len(hetero_inds) + 1) + qx)
                     for qx in np.arange(len(hetero_inds) + 1)]
            labels += ["$1$"] + ["$x_{{{}}}$".format(qx) for qx in hetero_inds]
    plt.xticks(locs, labels)
    plt.tight_layout()
```


# testing dynamicml -- this works with sample data
```{python}
import econml
from econml.panel.dml import DynamicDML
# from econml.tests.dgp import DynamicPanelDGP, add_vlines

# Helper imports
import numpy as np
from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV, MultiTaskLassoCV
import matplotlib.pyplot as plt

%matplotlib inline

# Define DGP parameters
np.random.seed(123)
n_panels = 5000 # number of panels
n_periods = 3 # number of time periods in each panel
n_treatments = 1 # number of treatments in each period
n_x = 100 # number of features + controls
s_x = 10 # number of controls (endogeneous variables)
s_t = 10 # treatment support size

dgp = DynamicPanelDGP(n_periods, n_treatments, n_x).create_instance(
            s_x, random_seed=12345)
Y, T, X, W, groups = dgp.observational_data(n_panels, s_t=s_t, random_seed=12345)
true_effect = dgp.true_effect

Y.shape
T.shape
W.shape
groups.shape

est = DynamicDML(
    model_y=LassoCV(cv=3, max_iter=1000), 
    model_t=MultiTaskLassoCV(cv=3, max_iter=1000), 
    cv=3)

est.fit(Y, T, X=None, W=W, groups=groups)

est.
```



# trying econml dynamic with my own sample data
```{python}
from IPython.display import Image, display
from dowhy import CausalModel
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.linear_model import LassoCV
# from sklearn.ensemble import GradientBoostingRegressor
from econml.dml import CausalForestDML
from econml.sklearn_extensions.linear_model import DebiasedLasso
from xgboost import XGBRegressor, XGBClassifier 
from econml.panel.dml import DynamicDML
import pandas as pd
import numpy as np

# from econml.tests.dgp import DynamicPanelDGP, add_vlines


dag_ex = 'digraph {a -> y; x -> y; a -> x; group -> a; group -> y; group -> x}'

group = np.array([1] * 8 + [2] * 8 + [3] * 8 + [4] * 8)
group = [1,1,1,1,1,1,np.nan,np.nan,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,4,4,4,4,4,4,np.nan,np.nan]
group = group.reshape(32)
W = np.random.randn(len(group))
T = np.random.randn(len(group))
Y = np.random.randn(len(group))
Y1 = np.random.randn(6)
Y2 = np.random.randn(8)
Ynan = [np.nan,np.nan]
Yzero = np.zeros(2)
Y = np.concatenate((Y1,Ynan,Y2, Y2, Y1, Ynan), axis=0)
Y = np.concatenate((Y1,Yzero,Y2,Y2,Y1,Yzero), axis=0)
data = {'group': group, 'a': a, 'x': x, 'y': y}
df = pd.DataFrame(data)



W0 = W[:,0]
W1 = W[:,1]
T = T.reshape(15000)

data = {'Y':Y,"T":T,"W":W,"group":group }

df = pd.DataFrame(data)

dag_ex = 'digraph {W -> Y; T -> Y; W -> T; group -> W; group -> Y; group -> X}'

model = CausalModel(
      data=df,
      treatment='T',
      outcome='Y',
      graph=dag_ex)

group2 = df.group

identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
identifier_name = identified_estimand.estimands

param = { "init_params":{'model_y': XGBRegressor(),
                              'model_t':XGBRegressor()},
          # 'model_final': DebiasedLasso(), # 'auto',
          #'featurizer':PolynomialFeatures(degree=1, include_bias=True)
                                
          #"fit_params":{},
          'discrete_treatment' : False,
          'fit_params' : { 'groups' : df['group'] }                     
        }

causal_estimate = model.estimate_effect(identified_estimand,
                                          method_name="backdoor.econml.panel.dml.DynamicDML",
                                          # test_significance=True,
                                          # confidence_intervals =True, 
                                          method_params=param)


est = DynamicDML(
    model_y=XGBRegressor(), 
    model_t=XGBRegressor())

T = T.reshape(32,1)
W = W.reshape(32,1)

est.fit(Y, T, X=None, W=W, groups=group, inference = "auto")

est.ate()
est.ate_interval()

Y.shape
T.shape
W.shape
group.shape
```

